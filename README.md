# 손말톡톡 | 농인과 청인간의 소통 지원 🤝
#### 숙명여자대학교 인공지능공학부 OSS_project 27팀(LuckY) 🍀

---

## 1. 프로젝트 소개 🖥️
수많은 수어 관련 프로젝트들을 봤지만, 단순하게 단어를 번역해주는 프로젝트들만 다수 존재할 뿐, 실시간으로 소통이 가능하도록 하는 프로젝트는 보지 못함.수어는 음성으로, 음성은 텍스트로 번역함으로서 하나의 단말기로 농인과 비장애인이 소통할 수 있도록 하고자 함.

---

## 2. 개발 과정 ⏰
✅ 기간: 2024. 05. 21 ~ 2024. 06. 20
- 아이디어 노트 작성
- 제안서 작성
- 학습 데이터 수집(aihub)
- 수집 데이터 전처리(동영상 -> 이미지)
- 이미지 학습 -> 모델 선정
- 모델 -> 서비스 구현

✅ Skills & Tools

---

## 3. 개발자 소개 📍
- **팀장 최예인(2310165)** : 학습 데이터 수집, 수집 데이터 전처리, 학습 데이터 적용
- **박예린(2311475)** : 학습 데이터 수집, 수집 데이터 전처리, 이미지 학습
- **신소연(2311787)** : 학습 데이터 수집, 수집 데이터 전처리, 이미지 학습
- **차서연(2314865)** : 학습 데이터 수집, 수집 데이터 전처리, 이미지 학습


![팀원소개](https://github.com/SMAI-LuckY/OSS_project/blob/main/%ED%8C%80%EC%9B%90%EC%86%8C%EA%B0%9C.png)

---

## 4. For Users

#### 1️⃣ 프로젝트 구성

#### 5️⃣ 버그 및 디버그

키포인트 학습 트러블 슈팅

문제 : 기존의계획은 AIHub에서 제공하는 keypoint를 사용해 모델을 학습시키고 결과값을 예측하려고 했으나, 동일한  영상에대해 mediapipe로 뽑은 keypoint와 Hub에서 제공하는 keypoint가 다르고, 프레임별로 keypoint가 존재하기 때문에 데이터의 양이 방대하여 학습에 많은 시간이 소요됨

해결 : keypoint의 형식을 맞추기 위해 주어진 keypoint를 사용하지 않고 직접 원천데이터에서 keypoint를 추출하고, 프레임별로 주어지는 point값들을 압축하기 위해 각 keypoint의 x, y, score 값을 매핑하여 하나의 이미지에 전체 프레임에 대한 정보를 담아 이를 학습시킴

#### 6️⃣ 참고 및 출처

https://github.com/DEVOCEAN-YOUNG-404/HandTalker 
- https://github.com/DEVOCEAN-YOUNG-404/HandTalker
- https://github.com/google-ai-edge/mediapipe
- https://vrworld.tistory.com/12

 



